# ML Model Performance Testing Guide

## âœ… Current Status

- **ML Model**: âœ… Working (tested directly)
- **API Integration**: âœ… Integrated
- **Logging**: âœ… Added to show ML vs rule-based comparison

## ðŸ§ª Test Results

### Direct ML Test
- **Status**: âœ… SUCCESS
- **Model Prediction**: 0.470 (47% probability)
- **Model File**: `models/match_compatibility_model.pkl` âœ…

### API Tests
- **Tests Run**: 5
- **Matches Found**: 11
- **ML Scores**: Check server logs (may need server restart)

## ðŸ“‹ How to Monitor ML Performance

### 1. Check Server Logs

When you make a search request, watch your **server terminal** (where `npm run dev` is running) for these log messages:

#### âœ… ML Scoring Active
```
ðŸ¤– ML Scoring: Rule-based=0.625, ML=0.470, Blended=0.519 (-0.106, -17.0%)
```

This shows:
- **Rule-based**: Traditional algorithm score
- **ML**: Pure ML model prediction
- **Blended**: Final score (70% ML + 30% rule-based)
- **Difference**: How much ML changed the score (+/-)
- **% Change**: Percentage change from rule-based

#### âš ï¸ ML Fallback
```
âš ï¸  ML scoring unavailable, using rule-based: 0.625
```

This means ML failed and fell back to rule-based scoring.

#### âŒ ML Errors
```
âš ï¸  ML prediction error: [error message]
âš ï¸  ML prediction spawn error: [error message]
```

These indicate issues with the Python ML service.

### 2. Test via API

Use the test script:
```bash
node test-ml-via-api.js
```

Or make direct API calls:
```bash
# Example
curl http://localhost:3000/api/match-solo?userId=seed_luxury_traveler_002
```

### 3. Test Direct ML Prediction

Test the ML model directly:
```bash
node test-ml-prediction-direct.js
```

## ðŸ“Š What to Look For

### Good Signs âœ…
- Logs show `ðŸ¤– ML Scoring:` messages
- ML scores are different from rule-based scores
- Blended scores are being calculated
- No error messages

### Warning Signs âš ï¸
- Only seeing `âš ï¸  ML scoring unavailable` messages
- All scores are the same (ML not working)
- Python errors in logs
- Timeout errors

## ðŸ”§ Troubleshooting

### If ML isn't working:

1. **Check Python is installed**
   ```bash
   python --version
   ```

2. **Check model file exists**
   ```bash
   ls models/match_compatibility_model.pkl
   ```

3. **Test ML directly**
   ```bash
   node test-ml-prediction-direct.js
   ```

4. **Restart server**
   - Stop the server (Ctrl+C)
   - Restart: `npm run dev`
   - Make a new search request

5. **Check server logs**
   - Look for Python errors
   - Look for path errors
   - Look for timeout errors

## ðŸ“ˆ Performance Metrics to Track

1. **ML Availability Rate**
   - How often ML succeeds vs falls back
   - Target: >80% success rate

2. **Score Differences**
   - Average difference between ML and rule-based
   - Positive = ML improves scores
   - Negative = ML decreases scores

3. **Match Quality**
   - Are ML-enhanced matches better?
   - User acceptance rates
   - Chat initiation rates

## ðŸŽ¯ Expected Behavior

- **ML should run** for every compatible match
- **Logs should show** ML vs rule-based comparison
- **Scores should differ** when ML is active
- **Fallback should work** if ML fails (no errors)

## ðŸ’¡ Tips

- **First time**: Restart server after ML integration
- **Monitoring**: Keep server logs open while testing
- **Comparison**: Note score differences to evaluate ML value
- **Data**: More training data = better ML performance
